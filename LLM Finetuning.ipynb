{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221d6cbf",
   "metadata": {},
   "source": [
    "# Basic Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ea4842-cfba-4f03-8964-85089213d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import AdamWeightDecay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829b60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data \n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22210807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Tokenization & Train-validation-test split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "def train_val_test_split(tokenized_dataset, test_size, val_size=0):\n",
    "        tokenized_dataset.shuffle()\n",
    "        temp1 = tokenized_dataset.train_test_split(test_size = test_size)\n",
    "        tokenized_test = temp1[\"test\"]\n",
    "        temp2 = temp1[\"train\"]\n",
    "        temp3 = temp2.train_test_split(test_size = val_size/(1-test_size))\n",
    "        tokenized_val = temp3[\"test\"]\n",
    "        tokenized_train = temp3[\"train\"]\n",
    "        return tokenized_train, tokenized_val, tokenized_test\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset_train, tokenized_dataset_val, tokenized_dataset_test =train_val_test_split(tokenized_dataset, test_size=0.2, val_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b267925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "##convert data to trainable data\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_dataset = tokenized_dataset_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_dataset_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_test_dataset = tokenized_dataset_test.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0ec9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## freeze the layers of the pretrained model\n",
    "def createTransferModel(base_model, freeze_n=0, freeze = True):\n",
    "    if freeze == True:\n",
    "        for i, layer in enumerate(base_model.layers[:-freeze_n]):\n",
    "            base_model.layers[i].trainable=False\n",
    "    else:\n",
    "        for i, layer in enumerate(base_model.layers[:]):\n",
    "            base_model.layers[i].trainable=True\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b7d05",
   "metadata": {},
   "source": [
    "training with freezed layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7127e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 22s 60ms/step - loss: 0.8620 - accuracy: 0.6222 - val_loss: 0.8043 - val_accuracy: 0.7196\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 8s 46ms/step - loss: 0.7662 - accuracy: 0.6598 - val_loss: 0.6894 - val_accuracy: 0.7528\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 8s 47ms/step - loss: 0.7008 - accuracy: 0.7025 - val_loss: 0.7019 - val_accuracy: 0.7064\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6875 - accuracy: 0.7091 - val_loss: 0.6018 - val_accuracy: 0.7417\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 0.6566 - accuracy: 0.7386 - val_loss: 0.6396 - val_accuracy: 0.7130\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6246 - accuracy: 0.7327 - val_loss: 0.5871 - val_accuracy: 0.7417\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 8s 46ms/step - loss: 0.6031 - accuracy: 0.7577 - val_loss: 0.5605 - val_accuracy: 0.7550\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.5978 - accuracy: 0.7622 - val_loss: 0.5487 - val_accuracy: 0.7506\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.6033 - accuracy: 0.7452 - val_loss: 0.5885 - val_accuracy: 0.7285\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 7s 42ms/step - loss: 0.5696 - accuracy: 0.7592 - val_loss: 0.5212 - val_accuracy: 0.7748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2471237fb20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import AdamWeightDecay\n",
    "from sklearn.metrics import accuracy_score\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "transfer_model = createTransferModel(model,1)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debf16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef4336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7748344370860927"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## report the accuracy score of the validation dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "preds_val = tf.nn.softmax(transfer_model.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d13fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7505518763796909"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## report the accuracy score of the test dataset\n",
    "preds_test = tf.nn.softmax(transfer_model.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c22cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "170/170 [==============================] - 31s 115ms/step - loss: 0.2997 - accuracy: 0.8881 - val_loss: 0.1417 - val_accuracy: 0.9514\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0733 - accuracy: 0.9742 - val_loss: 0.1371 - val_accuracy: 0.9558\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 18s 103ms/step - loss: 0.0433 - accuracy: 0.9890 - val_loss: 0.1387 - val_accuracy: 0.9536\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0166 - accuracy: 0.9934 - val_loss: 0.1175 - val_accuracy: 0.9669\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 0.1367 - val_accuracy: 0.9581\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.1189 - val_accuracy: 0.9669\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.1206 - val_accuracy: 0.9669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x246ff379970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## unfreeze all the layers and train with a smaller learning rate\n",
    "transfer_model_unfreezed = createTransferModel(transfer_model, freeze=False)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "transfer_model_unfreezed.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model_unfreezed.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 15,\n",
    "    callbacks = [callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6fc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9668874172185431"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val = tf.nn.softmax(transfer_model_unfreezed.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98aa5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9624724061810155"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = tf.nn.softmax(transfer_model_unfreezed.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c8a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 31s 115ms/step - loss: 0.2108 - accuracy: 0.9426 - val_loss: 0.1656 - val_accuracy: 0.9470\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0674 - accuracy: 0.9794 - val_loss: 0.2796 - val_accuracy: 0.9073\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.0942 - accuracy: 0.9705 - val_loss: 0.1410 - val_accuracy: 0.9470\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.1177 - accuracy: 0.9632 - val_loss: 0.1785 - val_accuracy: 0.9669\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.1576 - accuracy: 0.9558 - val_loss: 0.1531 - val_accuracy: 0.9536\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.0641 - accuracy: 0.9831 - val_loss: 0.4352 - val_accuracy: 0.9205\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0463 - accuracy: 0.9890 - val_loss: 0.2742 - val_accuracy: 0.9316\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.1007 - accuracy: 0.9698 - val_loss: 0.2348 - val_accuracy: 0.9470\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0560 - accuracy: 0.9853 - val_loss: 0.3702 - val_accuracy: 0.9272\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0927 - accuracy: 0.9779 - val_loss: 0.2253 - val_accuracy: 0.9360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a11980f70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Instead of head first then body, I train all the weights simultaneously from the begining.\n",
    "transfer_model_allw = createTransferModel(model,freeze=False)\n",
    "transfer_model_allw.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model_allw.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae66332d-7625-47d0-9964-90d85b8d5dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model_allw.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d532f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 5s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9359823399558499"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val = tf.nn.softmax(transfer_model_allw.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c7dde99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9359823399558499"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = tf.nn.softmax(transfer_model_allw.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a358b6c-7ce6-4049-abb3-3d12a7ec548f",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "In summary, I choose to use the pretrained model with checkpoint = \"bert-base-uncased\". I first freeze the layers of the pretrained model, and only the weights of the classification head can be trained. After fitting, the accuracy score on the validation dataset is around 77%, and the accuracy score on the test dataset is around 74%. We can see from the training process that the validation accuracy is increasing, but it can only reach around 80% at maximum. I then unfreeze all the layers of the model to enable training, with a smaller learning rate. The reasoning is that after getting the weights of the classifier head, the weights of the previously-freezed layers can be \"fine-tuned\" for our dataset, which are not supposed to change much. Amazingly, with all layers being trainable, the accuracy score on the validation dataset is around 96% and the accuracy score on the test dataset is around 96%.\n",
    "\n",
    "Next, I also try another pre-trained model from the same checkpoint, but with no layer freezed at first. Namely, I train all the weights of the bert model as well as of the classification head simultaneously. The model's performance varies. In the case above, its performance turns out good, with a validation accuracy score of 94% and a test accuracy score of around 96%, even though it is not as good as the first approach. However, in some other cases of the training, with the same hyperparameters, the model could not improve its performance in terms of validation accuracy. The potential problem here is that the pretrained-model has an extremely huge amount of parameters. All layers being unfreezed simply gives too much freedom to the training process, and makes the training unstable, the model hard to find the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd54b86",
   "metadata": {},
   "source": [
    "# Extra parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ffc47",
   "metadata": {},
   "source": [
    "## Create (and fit the model with) a TensorFlow Dataset (TFDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1cef42-d57d-47c2-9883-09d7de641f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f395cb0d6d1f4cf5be5418f4c5fe502c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\")\n",
    "dataset_build = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", split = \"train\")\n",
    "tokenized_dataset_build = dataset_build.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb38c8d9-6a8e-485b-8441-250daa18440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_get_batch(\n",
    "    indices, dataset, cols_to_retain, collate_fn, collate_fn_args, columns_to_np_types, return_dict=False\n",
    "):\n",
    "    if not isinstance(indices, np.ndarray):\n",
    "        indices = indices.numpy()\n",
    "\n",
    "    is_batched = True\n",
    "    if isinstance(indices, np.integer):\n",
    "        batch = dataset[indices.item()]\n",
    "        is_batched = False\n",
    "    elif np.all(np.diff(indices) == 1):\n",
    "        batch = dataset[indices[0] : indices[-1] + 1]\n",
    "    elif isinstance(indices, np.ndarray):\n",
    "        batch = dataset[indices]\n",
    "    else:\n",
    "        raise RuntimeError(\"Unexpected type for indices: {}\".format(type(indices)))\n",
    "\n",
    "    if cols_to_retain is not None:\n",
    "        batch = {\n",
    "            key: value\n",
    "            for key, value in batch.items()\n",
    "            if key in cols_to_retain or key in (\"label\", \"label_ids\", \"labels\")\n",
    "        }\n",
    "\n",
    "    if is_batched:\n",
    "        actual_size = len(list(batch.values())[0])  # Get the length of one of the arrays, assume all same\n",
    "        # Our collators expect a list of dicts, not a dict of lists/arrays, so we invert\n",
    "        batch = [{key: value[i] for key, value in batch.items()} for i in range(actual_size)]\n",
    "    batch = collate_fn(batch, **collate_fn_args)\n",
    "\n",
    "    if return_dict:\n",
    "        out_batch = {}\n",
    "        for col, cast_dtype in columns_to_np_types.items():\n",
    "            # In case the collate_fn returns something strange\n",
    "            array = np.array(batch[col])\n",
    "            array = array.astype(cast_dtype)\n",
    "            out_batch[col] = array\n",
    "    else:\n",
    "        out_batch = []\n",
    "        for col, cast_dtype in columns_to_np_types.items():\n",
    "            # In case the collate_fn returns something strange\n",
    "            array = np.array(batch[col])\n",
    "            array = array.astype(cast_dtype)\n",
    "            out_batch.append(array)\n",
    "    return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8363ebfd-6e2e-4db1-a340-17986a873eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = {'input_ids': tf.TensorSpec(shape=(None, None, None), dtype=tf.int64, name=None),\n",
    "  'token_type_ids': tf.TensorSpec(shape=(None, None, None), dtype=tf.int64, name=None),\n",
    "  'attention_mask': tf.TensorSpec(shape=(None, None, None), dtype=tf.int64, name=None),\n",
    "  'labels': tf.TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}\n",
    "columns_to_np_types = {'input_ids': np.int64,\n",
    "  'token_type_ids': np.int64,\n",
    "  'attention_mask': np.int64,\n",
    "  'labels': np.int64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2af1528-28f3-460b-8cd2-cde8d3c36c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "getter_fn = partial(\n",
    "        np_get_batch,\n",
    "        dataset=tokenized_dataset_build,\n",
    "        cols_to_retain=[\"attention_mask\", \"input_ids\", \"token_type_ids\", \"labels\"],\n",
    "        collate_fn=data_collator,\n",
    "        collate_fn_args={},\n",
    "        columns_to_np_types=columns_to_np_types,\n",
    "        return_dict=False,\n",
    "    )\n",
    "tout = [tf.dtypes.as_dtype(dtype) for dtype in columns_to_np_types.values()]\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(None, tf.int64)])\n",
    "def fetch_function(indices):\n",
    "    output = tf.py_function(\n",
    "        getter_fn,\n",
    "        inp=[indices],\n",
    "        Tout=tout,\n",
    "    )\n",
    "    return {key: output[i] for i, key in enumerate(columns_to_np_types.keys())}\n",
    "\n",
    "tf_dataset = tf.data.Dataset.range(len(dataset))\n",
    "\n",
    "tf_dataset = tf_dataset.map(fetch_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17682309-2bee-4085-ba8c-e9d21a0294e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_shapes(input_dict):\n",
    "        return {key: tf.ensure_shape(val, output_signature[key].shape[1:]) for key, val in input_dict.items()}\n",
    "    \n",
    "class Operation:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.dataset = self.dataset.shuffle(buffer_size = self.dataset.cardinality())\n",
    "        return self.dataset\n",
    "\n",
    "    def batch(self, batch_size):\n",
    "        self.dataset = self.dataset.batch(batch_size, drop_remainder=True)\n",
    "        return self.dataset\n",
    "\n",
    "    def train_test_split(self, train_size):\n",
    "        dataset1 = self.dataset.take(train_size)\n",
    "        dataset2 = self.dataset.skip(train_size)\n",
    "        return dataset1.map(ensure_shapes), dataset2.map(ensure_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40bf9ce-d378-4aa6-9537-0ec6a340cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = Operation(tf_dataset)\n",
    "shuffled_dataset = operation.shuffle()\n",
    "batched_dataset = operation.batch(8)\n",
    "train_dataset, test_dataset = operation.train_test_split(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff34708a-3804-4c74-8850-2427ac0bd119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'labels': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "721fc892-97fc-4f51-8e34-1ed7e25d5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "283/283 [==============================] - 60s 150ms/step - loss: 0.8371 - accuracy: 0.6427\n",
      "Epoch 2/10\n",
      "283/283 [==============================] - 49s 151ms/step - loss: 0.7101 - accuracy: 0.7045\n",
      "Epoch 3/10\n",
      "283/283 [==============================] - 48s 149ms/step - loss: 0.6422 - accuracy: 0.7257\n",
      "Epoch 4/10\n",
      "283/283 [==============================] - 47s 149ms/step - loss: 0.6327 - accuracy: 0.7319\n",
      "Epoch 5/10\n",
      "283/283 [==============================] - 47s 148ms/step - loss: 0.6149 - accuracy: 0.7359\n",
      "Epoch 6/10\n",
      "283/283 [==============================] - 47s 148ms/step - loss: 0.5807 - accuracy: 0.7531\n",
      "Epoch 7/10\n",
      "283/283 [==============================] - 48s 148ms/step - loss: 0.5696 - accuracy: 0.7602\n",
      "Epoch 8/10\n",
      "283/283 [==============================] - 47s 149ms/step - loss: 0.5783 - accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "283/283 [==============================] - 47s 148ms/step - loss: 0.5696 - accuracy: 0.7553\n",
      "Epoch 10/10\n",
      "283/283 [==============================] - 48s 149ms/step - loss: 0.5465 - accuracy: 0.7610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10bc7e91ac0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "transfer_model = createTransferModel(model,1)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model.fit(\n",
    "    train_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df540c8-deb9-4055-adb3-48783424904a",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "I build the tensorflow dataset from scratch, and then I write an class \"operation\" to shuffle, batch, and split the dataset. One thing I have taken care of the shape of the dataset after batching/splitting. In order for the dataset to be fed into the model, I removed the batch/take/skip dimension so that the tensor shape are compatible with what the model requires. Also, I choose to pad the data with max length, which might slow down the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b4972",
   "metadata": {},
   "source": [
    "## Create your own Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a364fe-22c2-4077-b5b8-0108472c9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "import keras\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import AdamWeightDecay\n",
    "        \n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bertlayer = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dropout = keras.layers.Dropout(0.1)\n",
    "        self.dense2 = keras.layers.Dense(3, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "  \n",
    "        inputs1 = self.bertlayer(input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"], token_type_ids = inputs[\"token_type_ids\"])\n",
    "        inputs2 = self.dropout(inputs1[1])\n",
    "        inputs3= self.dense1(inputs2)\n",
    "        return self.dense2(inputs3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9aef20-6d6e-4116-b8f5-d73615721894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 21s 59ms/step - loss: 0.8861 - accuracy: 0.6267 - val_loss: 0.7385 - val_accuracy: 0.6490\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.7424 - accuracy: 0.6797 - val_loss: 0.6311 - val_accuracy: 0.7417\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6875 - accuracy: 0.7084 - val_loss: 0.5884 - val_accuracy: 0.7550\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6403 - accuracy: 0.7165 - val_loss: 0.5982 - val_accuracy: 0.7572\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 0.6572 - accuracy: 0.7121 - val_loss: 0.5349 - val_accuracy: 0.7660\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6129 - accuracy: 0.7231 - val_loss: 0.5276 - val_accuracy: 0.7594\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 7s 44ms/step - loss: 0.6006 - accuracy: 0.7342 - val_loss: 0.5582 - val_accuracy: 0.7219\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.5952 - accuracy: 0.7349 - val_loss: 0.5024 - val_accuracy: 0.8013\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 7s 44ms/step - loss: 0.6051 - accuracy: 0.7283 - val_loss: 0.5011 - val_accuracy: 0.8013\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 0.5684 - accuracy: 0.7445 - val_loss: 0.5030 - val_accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1329109dd00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_own = MyModel()\n",
    "model_own_freezed = createTransferModel(model_own,3)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_own_freezed.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model_own_freezed.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a9d702-6273-419c-8c2d-a76462a0d502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 31s 116ms/step - loss: 0.4361 - accuracy: 0.8270 - val_loss: 0.3719 - val_accuracy: 0.8389\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.2420 - accuracy: 0.9212 - val_loss: 0.3380 - val_accuracy: 0.9007\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.1790 - accuracy: 0.9426 - val_loss: 0.3168 - val_accuracy: 0.9338\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.1492 - accuracy: 0.9470 - val_loss: 0.3494 - val_accuracy: 0.9272\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.1408 - accuracy: 0.9602 - val_loss: 0.2090 - val_accuracy: 0.9426\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.1280 - accuracy: 0.9669 - val_loss: 0.2772 - val_accuracy: 0.9117\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.1429 - accuracy: 0.9617 - val_loss: 0.2761 - val_accuracy: 0.9205\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.0823 - accuracy: 0.9772 - val_loss: 0.2970 - val_accuracy: 0.9360\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.1122 - accuracy: 0.9705 - val_loss: 0.2367 - val_accuracy: 0.9382\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.1623 - accuracy: 0.9543 - val_loss: 0.2863 - val_accuracy: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132618000d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_own_unfreezed = createTransferModel(model_own_freezed, freeze=False)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model_own_unfreezed.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model_own_unfreezed.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d11ab4-2525-4a7d-88d4-8a2eb243064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9139072847682119"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "preds_val = model_own_freezed.predict(tf_validation_dataset)\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3fa55a-e0d8-4904-8979-4d3ae77eada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9116997792494481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = model_own_freezed.predict(tf_test_dataset)\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cab4d4-ca18-436a-bb6d-47442e314c71",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "The key insight is that the pre-trained BERT model outputs contextualized token representations from the [CLS] special token. This output can serve as features that capture the semantic meaning of the input text. We can feed that into our own classifier head to make predictions. First, when loading the pre-trained BERT model using TFBertModel.from_pretrained, this model does not have any prediction head. It just outputs token embeddings. We first feed the [CLS] embedding into a Dense layer to reduce the dimensionality and introduce some nonlinearity with ReLU activation. The dropout layer regularizes the model. Finally, the last Dense layer has an output size of 3, to make predictions for the 3 classes in our dataset, with a softmax activation to output prediction probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297c1c5",
   "metadata": {},
   "source": [
    "## 2.3. Use different \"flavors\" of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "850be363-6f83-4c5f-accb-6b24f23fe99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer():\n",
    "    def __init__(self, dataset, checkpoint):\n",
    "        self.dataset = dataset.shuffle(seed=42)\n",
    "        self.checkpoint = checkpoint\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        \n",
    "    def tokenize_function(self, example):\n",
    "        return self.tokenizer(example[\"sentence\"], truncation=True, padding=True)\n",
    "\n",
    "    def get_tokenized_data(self):\n",
    "        tokenized_dataset = self.dataset.map(self.tokenize_function, batched=True)\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def train_val_test_split(self, test_size, val_size=0):\n",
    "        tokenized_dataset = self.get_tokenized_data()\n",
    "        temp1 = tokenized_dataset.train_test_split(test_size = test_size)\n",
    "        tokenized_test = temp1[\"test\"]\n",
    "        temp2 = temp1[\"train\"]\n",
    "        temp3 = temp2.train_test_split(test_size = val_size/(1-test_size))\n",
    "        tokenized_val = temp3[\"test\"]\n",
    "        tokenized_train = temp3[\"train\"]\n",
    "        return tokenized_train, tokenized_val, tokenized_test\n",
    "\n",
    "\n",
    "    def train(self, model, tokenized_dataset_train, tokenized_dataset_val=None,freeze_layer=0, epochs=10, unfreeze_after=False):\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "        # tokenized_dataset_train, tokenized_dataset_val, tokenized_dataset_test = self.train_val_test_split(test_size=test_size, val_size = val_size)\n",
    "        tf_train_dataset = tokenized_dataset_train.to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "            label_cols=[\"label\"],\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            batch_size=8,\n",
    "        )\n",
    "\n",
    "        tf_val_dataset = tokenized_dataset_val.to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "            label_cols=[\"label\"],\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "            batch_size=8,\n",
    "        )\n",
    "\n",
    "        transfer_model = createTransferModel(model,freeze_n=freeze_layer)\n",
    "        transfer_model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        transfer_model.fit(\n",
    "            tf_train_dataset,\n",
    "            validation_data=tf_val_dataset,\n",
    "            epochs = epochs, \n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "        )\n",
    "        self.trained_model = transfer_model\n",
    "        if unfreeze_after == True:\n",
    "            transfer_model_unfreezed = createTransferModel(transfer_model, freeze=False)\n",
    "            transfer_model_unfreezed.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[\"accuracy\"],\n",
    "            )\n",
    "            transfer_model_unfreezed.fit(\n",
    "                tf_train_dataset,\n",
    "                validation_data=tf_val_dataset,\n",
    "                epochs = epochs,\n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "            )\n",
    "            self.trained_model = transfer_model_unfreezed\n",
    "        return \n",
    "\n",
    "        \n",
    "    def test_score(self, tokenized_dataset_test):\n",
    "\n",
    "        tf_test_dataset = tokenized_dataset_test.to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "            label_cols=[\"label\"],\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        \n",
    "        preds_test = tf.nn.softmax(self.trained_model.predict(tf_test_dataset)[\"logits\"])\n",
    "        class_preds_test = np.argmax(preds_test, axis=1)\n",
    "# print(len(tokenized_dataset_val[\"label\"]),len(class_preds_test))\n",
    "        accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "        return accuracy_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42223728",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", split = \"train\")\n",
    "dataset_75 = load_dataset(\"financial_phrasebank\", \"sentences_75agree\", split = \"train\")\n",
    "dataset_66 = load_dataset(\"financial_phrasebank\", \"sentences_66agree\", split = \"train\")\n",
    "dataset_50 = load_dataset(\"financial_phrasebank\", \"sentences_50agree\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7281450-5cc4-4df2-953b-a502e90e9997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346bd4de766a48d095f22f9e6c70512d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 18s 58ms/step - loss: 0.8741 - accuracy: 0.6289 - val_loss: 0.7717 - val_accuracy: 0.6468\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.7705 - accuracy: 0.6561 - val_loss: 0.7242 - val_accuracy: 0.7285\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.7137 - accuracy: 0.6966 - val_loss: 0.6683 - val_accuracy: 0.7152\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6663 - accuracy: 0.7202 - val_loss: 0.7000 - val_accuracy: 0.6689\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.6306 - accuracy: 0.7452 - val_loss: 0.6153 - val_accuracy: 0.7638\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.6240 - accuracy: 0.7378 - val_loss: 0.6003 - val_accuracy: 0.7373\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.6059 - accuracy: 0.7467 - val_loss: 0.5787 - val_accuracy: 0.7748\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 7s 44ms/step - loss: 0.5930 - accuracy: 0.7592 - val_loss: 0.6778 - val_accuracy: 0.7241\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.5961 - accuracy: 0.7482 - val_loss: 0.6110 - val_accuracy: 0.7351\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 7s 43ms/step - loss: 0.5750 - accuracy: 0.7423 - val_loss: 0.5775 - val_accuracy: 0.7439\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 32s 114ms/step - loss: 0.3779 - accuracy: 0.8741 - val_loss: 0.1955 - val_accuracy: 0.9404\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.1679 - accuracy: 0.9492 - val_loss: 0.1471 - val_accuracy: 0.9470\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.0807 - accuracy: 0.9735 - val_loss: 0.2274 - val_accuracy: 0.9404\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.0698 - accuracy: 0.9816 - val_loss: 0.1500 - val_accuracy: 0.9647\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.0748 - accuracy: 0.9794 - val_loss: 0.1978 - val_accuracy: 0.9514\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "Trainer_all = trainer(dataset = dataset_all, checkpoint = checkpoint)\n",
    "tokenized_dataset_train_all, tokenized_dataset_val_all, tokenized_dataset_test_all = Trainer_all.train_val_test_split(test_size=0.2, val_size=0.2)\n",
    "Trainer_all.train(model = model,tokenized_dataset_train=tokenized_dataset_train_all, tokenized_dataset_val=tokenized_dataset_val_all,freeze_layer=1, epochs=10, unfreeze_after=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d154304-dfea-4033-b504-66b802b45822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9646799116997793"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_all.test_score(tokenized_dataset_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3beef0-9832-47a6-aa66-07e299ce3668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c8bf1368a415aa156e0f3ae611fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 23s 52ms/step - loss: 0.8523 - accuracy: 0.6374 - val_loss: 0.7760 - val_accuracy: 0.7164\n",
      "Epoch 2/10\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.7936 - accuracy: 0.6634 - val_loss: 0.7079 - val_accuracy: 0.6816\n",
      "Epoch 3/10\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.7428 - accuracy: 0.6823 - val_loss: 0.6902 - val_accuracy: 0.7164\n",
      "Epoch 4/10\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.7047 - accuracy: 0.6968 - val_loss: 0.7057 - val_accuracy: 0.7106\n",
      "Epoch 5/10\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.6784 - accuracy: 0.7050 - val_loss: 0.7946 - val_accuracy: 0.6397\n",
      "Epoch 6/10\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.6633 - accuracy: 0.7228 - val_loss: 0.6728 - val_accuracy: 0.6874\n",
      "Epoch 7/10\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.6601 - accuracy: 0.7084 - val_loss: 0.6541 - val_accuracy: 0.7250\n",
      "Epoch 8/10\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.6436 - accuracy: 0.7156 - val_loss: 0.6190 - val_accuracy: 0.7627\n",
      "Epoch 9/10\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.6446 - accuracy: 0.7175 - val_loss: 0.6039 - val_accuracy: 0.7323\n",
      "Epoch 10/10\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.6326 - accuracy: 0.7272 - val_loss: 0.5830 - val_accuracy: 0.7511\n",
      "Epoch 1/10\n",
      "259/259 [==============================] - 41s 110ms/step - loss: 0.4563 - accuracy: 0.8368 - val_loss: 0.3173 - val_accuracy: 0.8842\n",
      "Epoch 2/10\n",
      "259/259 [==============================] - 26s 101ms/step - loss: 0.1935 - accuracy: 0.9425 - val_loss: 0.2418 - val_accuracy: 0.9247\n",
      "Epoch 3/10\n",
      "259/259 [==============================] - 26s 99ms/step - loss: 0.1251 - accuracy: 0.9691 - val_loss: 0.3879 - val_accuracy: 0.8886\n",
      "Epoch 4/10\n",
      "259/259 [==============================] - 26s 100ms/step - loss: 0.1495 - accuracy: 0.9517 - val_loss: 0.3809 - val_accuracy: 0.9059\n",
      "Epoch 5/10\n",
      "259/259 [==============================] - 26s 101ms/step - loss: 0.1425 - accuracy: 0.9604 - val_loss: 0.3047 - val_accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "Trainer_75 = trainer(dataset = dataset_75, checkpoint = checkpoint)\n",
    "tokenized_dataset_train_75, tokenized_dataset_val_75, tokenized_dataset_test_75 = Trainer_75.train_val_test_split(test_size=0.2, val_size=0.2)\n",
    "Trainer_75.train(model = model,tokenized_dataset_train=tokenized_dataset_train_75, tokenized_dataset_val=tokenized_dataset_val_75, freeze_layer=1, epochs=10, unfreeze_after=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c75c154-c997-4bb7-ad68-287e3489a445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 5s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8900144717800289"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_75.test_score(tokenized_dataset_test_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8501b242-3e52-4529-80c3-f470c85d037a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693da678bf6a48dc80d27bce02362d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 26s 51ms/step - loss: 0.9000 - accuracy: 0.5998 - val_loss: 0.8342 - val_accuracy: 0.6588\n",
      "Epoch 2/10\n",
      "317/317 [==============================] - 14s 44ms/step - loss: 0.8260 - accuracy: 0.6362 - val_loss: 0.7515 - val_accuracy: 0.6801\n",
      "Epoch 3/10\n",
      "317/317 [==============================] - 14s 43ms/step - loss: 0.7891 - accuracy: 0.6465 - val_loss: 0.7783 - val_accuracy: 0.6825\n",
      "Epoch 4/10\n",
      "317/317 [==============================] - 14s 43ms/step - loss: 0.7701 - accuracy: 0.6552 - val_loss: 0.7188 - val_accuracy: 0.6896\n",
      "Epoch 5/10\n",
      "317/317 [==============================] - 14s 44ms/step - loss: 0.7409 - accuracy: 0.6769 - val_loss: 0.6797 - val_accuracy: 0.7204\n",
      "Epoch 6/10\n",
      "317/317 [==============================] - 14s 43ms/step - loss: 0.7176 - accuracy: 0.6718 - val_loss: 0.6650 - val_accuracy: 0.7192\n",
      "Epoch 7/10\n",
      "317/317 [==============================] - 14s 43ms/step - loss: 0.7241 - accuracy: 0.6726 - val_loss: 0.6847 - val_accuracy: 0.6848\n",
      "Epoch 8/10\n",
      "317/317 [==============================] - 14s 43ms/step - loss: 0.7037 - accuracy: 0.6773 - val_loss: 0.6534 - val_accuracy: 0.7133\n",
      "Epoch 9/10\n",
      "317/317 [==============================] - 14s 43ms/step - loss: 0.7108 - accuracy: 0.6860 - val_loss: 0.6530 - val_accuracy: 0.7145\n",
      "Epoch 10/10\n",
      "317/317 [==============================] - 14s 44ms/step - loss: 0.7171 - accuracy: 0.6785 - val_loss: 0.6963 - val_accuracy: 0.6836\n",
      "Epoch 1/10\n",
      "317/317 [==============================] - 47s 107ms/step - loss: 0.5347 - accuracy: 0.8035 - val_loss: 0.4179 - val_accuracy: 0.8602\n",
      "Epoch 2/10\n",
      "317/317 [==============================] - 32s 100ms/step - loss: 0.2741 - accuracy: 0.9095 - val_loss: 0.3390 - val_accuracy: 0.8720\n",
      "Epoch 3/10\n",
      "317/317 [==============================] - 32s 100ms/step - loss: 0.1779 - accuracy: 0.9427 - val_loss: 0.4937 - val_accuracy: 0.8566\n",
      "Epoch 4/10\n",
      "317/317 [==============================] - 31s 99ms/step - loss: 0.1586 - accuracy: 0.9502 - val_loss: 0.5999 - val_accuracy: 0.8424\n",
      "Epoch 5/10\n",
      "317/317 [==============================] - 31s 99ms/step - loss: 0.1150 - accuracy: 0.9711 - val_loss: 0.6716 - val_accuracy: 0.8282\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "Trainer_66 = trainer(dataset = dataset_66, checkpoint = checkpoint)\n",
    "tokenized_dataset_train_66, tokenized_dataset_val_66, tokenized_dataset_test_66 = Trainer_66.train_val_test_split(test_size=0.2, val_size=0.2)\n",
    "Trainer_66.train(model = model,tokenized_dataset_train=tokenized_dataset_train_66, tokenized_dataset_val=tokenized_dataset_val_66, freeze_layer=1, epochs=10, unfreeze_after=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60b3b07-d2eb-48f7-a5d7-276b77f67f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 6s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8281990521327014"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_66.test_score(tokenized_dataset_test_66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94529d8d-0a56-4f96-8dbf-0592b876396b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4453cea73ae454aba1101a55999883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 27s 49ms/step - loss: 0.8990 - accuracy: 0.5920 - val_loss: 0.8690 - val_accuracy: 0.6264\n",
      "Epoch 2/10\n",
      "364/364 [==============================] - 15s 42ms/step - loss: 0.8417 - accuracy: 0.6051 - val_loss: 0.8080 - val_accuracy: 0.6615\n",
      "Epoch 3/10\n",
      "364/364 [==============================] - 15s 42ms/step - loss: 0.8034 - accuracy: 0.6305 - val_loss: 0.7969 - val_accuracy: 0.6429\n",
      "Epoch 4/10\n",
      "364/364 [==============================] - 15s 41ms/step - loss: 0.7721 - accuracy: 0.6553 - val_loss: 0.8018 - val_accuracy: 0.6264\n",
      "Epoch 5/10\n",
      "364/364 [==============================] - 15s 42ms/step - loss: 0.7670 - accuracy: 0.6543 - val_loss: 0.7600 - val_accuracy: 0.6594\n",
      "Epoch 6/10\n",
      "364/364 [==============================] - 15s 42ms/step - loss: 0.7522 - accuracy: 0.6557 - val_loss: 0.7542 - val_accuracy: 0.6594\n",
      "Epoch 7/10\n",
      "364/364 [==============================] - 15s 41ms/step - loss: 0.7494 - accuracy: 0.6670 - val_loss: 0.7252 - val_accuracy: 0.6821\n",
      "Epoch 8/10\n",
      "364/364 [==============================] - 15s 41ms/step - loss: 0.7373 - accuracy: 0.6653 - val_loss: 0.8681 - val_accuracy: 0.6398\n",
      "Epoch 9/10\n",
      "364/364 [==============================] - 15s 41ms/step - loss: 0.7539 - accuracy: 0.6625 - val_loss: 0.7106 - val_accuracy: 0.6739\n",
      "Epoch 10/10\n",
      "364/364 [==============================] - 15s 41ms/step - loss: 0.7227 - accuracy: 0.6729 - val_loss: 0.7329 - val_accuracy: 0.6533\n",
      "Epoch 1/10\n",
      "364/364 [==============================] - 50s 103ms/step - loss: 0.6421 - accuracy: 0.7337 - val_loss: 0.5243 - val_accuracy: 0.7843\n",
      "Epoch 2/10\n",
      "364/364 [==============================] - 35s 97ms/step - loss: 0.3726 - accuracy: 0.8638 - val_loss: 0.5085 - val_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "364/364 [==============================] - 35s 97ms/step - loss: 0.2484 - accuracy: 0.9181 - val_loss: 0.5700 - val_accuracy: 0.8111\n",
      "Epoch 4/10\n",
      "364/364 [==============================] - 35s 96ms/step - loss: 0.2193 - accuracy: 0.9219 - val_loss: 0.7342 - val_accuracy: 0.8101\n",
      "Epoch 5/10\n",
      "364/364 [==============================] - 35s 97ms/step - loss: 0.1603 - accuracy: 0.9456 - val_loss: 0.8818 - val_accuracy: 0.7750\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "Trainer_50 = trainer(dataset = dataset_50,  checkpoint = checkpoint)\n",
    "tokenized_dataset_train_50, tokenized_dataset_val_50, tokenized_dataset_test_50 = Trainer_50.train_val_test_split(test_size=0.2, val_size=0.2)\n",
    "Trainer_50.train(model = model,tokenized_dataset_train=tokenized_dataset_train_50, tokenized_dataset_val=tokenized_dataset_val_50, freeze_layer=1, epochs=10, unfreeze_after=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c40d26ad-4f02-4d04-90ba-c827df78febb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 6s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7649484536082474"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_50.test_score(tokenized_dataset_test_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69c89a-80fb-4bb1-8be0-78038d70dc07",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "I apply the same model, the same training process, and the same hyperparameters separately to the four datasets, which are all-agreed, 75_agreed, 66_agreed, and 50-agreed. As we could see from the results, the test accuracy score declines with the quality of the datasets. \n",
    "\n",
    "The declining performance on the datasets with lower inter-annotator agreement makes sense, as the \"correct\" labels become more ambiguous and debatable as agreement drops. The all-agreed dataset likely has very clear, unambiguous labels that are easy to learn and generalize. There is no debate about what the correct label should be, making it easier for the model to recognize the patterns that correlate with each class. As agreement declines to 75%, there is more variability in the labels. For around 25% of examples, annotators disagreed on the best label. This introduces some noisy, potentially inconsistent labels into the training data, making the patterns harder to recognize. Performance declines, but there is still relatively high agreement. At 66% agreement, over 30% of the labels may be debatable. Different annotators can reasonably assign different labels to a substantial minority of examples. This makes it quite challenging for the model to properly learn the distinctions between classes. With only 50% agreement, the dataset likely contains a large fraction of examples that different people would legitimately label differently. Many examples likely have ambiguous qualities or lack strong cues that clearly differentiate the classes. This high level of subjectivity greatly reduces how learnable the categories are.\n",
    "\n",
    "In essence, less agreement means more noisy, subjective, borderline examples. This reduces how clearly defined the class patterns are in the data, making generalization more difficult. The model likely learns superficial cues and struggles to match the wisdom of the crowd on ambiguous cases. Augmenting training data, handling label uncertainty, and other techniques may help address such issues. But fundamentally, subjective and unclear labels limit model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942a422",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Address any Imbalanced Data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e5e41f-78d2-4147-b31e-38f18c56c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the proportion of each class\n",
    "label = tokenized_dataset_train[\"label\"]\n",
    "classes, count = np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c43be61e-9d32-41af-93f6-5a4f76c68106",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = count[0]\n",
    "neu = count[1]\n",
    "pos = count[2]\n",
    "total = neg + neu + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6e41993-c1c4-4882-bd2b-3db7e6debd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 7.26\n",
      "Weight for class 1: 1.63\n",
      "Weight for class 2: 4.02\n"
     ]
    }
   ],
   "source": [
    "weight_for_0 = (1 / neg) * (total)\n",
    "weight_for_1 = (1 / neu) * (total)\n",
    "weight_for_2 = (1 / pos) * (total)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97be9f0d-cb69-4472-aec7-d91cbc904efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 19s 61ms/step - loss: 3.2162 - accuracy: 0.4536 - val_loss: 1.2596 - val_accuracy: 0.2759\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 2.8733 - accuracy: 0.6053 - val_loss: 1.0248 - val_accuracy: 0.4327\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 2.6982 - accuracy: 0.6451 - val_loss: 0.7376 - val_accuracy: 0.7108\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 2.5343 - accuracy: 0.6686 - val_loss: 0.6966 - val_accuracy: 0.6954\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 2.5412 - accuracy: 0.6708 - val_loss: 0.7998 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 2.4123 - accuracy: 0.6988 - val_loss: 0.6850 - val_accuracy: 0.7196\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 7s 44ms/step - loss: 2.3539 - accuracy: 0.6944 - val_loss: 0.6358 - val_accuracy: 0.7130\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 2.3337 - accuracy: 0.7084 - val_loss: 0.5998 - val_accuracy: 0.7196\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 2.2875 - accuracy: 0.7003 - val_loss: 0.6642 - val_accuracy: 0.6998\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 2.2546 - accuracy: 0.7128 - val_loss: 0.7560 - val_accuracy: 0.6733\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 30s 113ms/step - loss: 1.2901 - accuracy: 0.8608 - val_loss: 0.2359 - val_accuracy: 0.9294\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.6829 - accuracy: 0.9300 - val_loss: 0.4431 - val_accuracy: 0.8918\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.5684 - accuracy: 0.9485 - val_loss: 0.1999 - val_accuracy: 0.9360\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.4312 - accuracy: 0.9639 - val_loss: 0.3008 - val_accuracy: 0.9338\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.1648 - accuracy: 0.9867 - val_loss: 0.4284 - val_accuracy: 0.9205\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.1947 - accuracy: 0.9809 - val_loss: 0.4628 - val_accuracy: 0.9029\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 17s 100ms/step - loss: 0.5348 - accuracy: 0.9507 - val_loss: 0.2638 - val_accuracy: 0.9249\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.6165 - accuracy: 0.9455 - val_loss: 0.8096 - val_accuracy: 0.8389\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 17s 102ms/step - loss: 0.3586 - accuracy: 0.9720 - val_loss: 0.4948 - val_accuracy: 0.8786\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 17s 101ms/step - loss: 0.5607 - accuracy: 0.9448 - val_loss: 0.3822 - val_accuracy: 0.9029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2072bb93b20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "transfer_model_imbalanced = createTransferModel(model,1)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model_imbalanced.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model_imbalanced.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10,\n",
    "    class_weight=class_weight\n",
    ")\n",
    "transfer_model_imbalanced_unfreezed = createTransferModel(transfer_model_imbalanced,freeze=False)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model_imbalanced_unfreezed.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model_imbalanced_unfreezed.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10,\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6e5d27b-a1d5-4edd-a0dc-093be6bcca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9028697571743929"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "preds_val = transfer_model_imbalanced_unfreezed.predict(tf_validation_dataset)\n",
    "class_preds_val = np.argmax(preds_val.logits, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd93e479-b142-437f-9d9d-c5f768a27afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8962472406181016"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = transfer_model_imbalanced_unfreezed.predict(tf_test_dataset)\n",
    "class_preds_test = np.argmax(preds_test.logits, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76388154-b378-476a-bccc-6d324719143f",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "To deal with the potential data imbalance, I first take a look at the proportion of each class takes in the data. It turns out that in training data, there exist data imbalance with neutral class taking up to 61%. Therefore, I pass the class weights argument to the model fitting. Class weigght is an  optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b809d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Superior Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab65088-fefb-4805-b4ec-24fd9ef000a3",
   "metadata": {},
   "source": [
    "### Is one class harder to correctly classify than the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e76f243e-1b32-41ea-bd12-bcef6fecbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_wrong_class(pred, true):\n",
    "    count_wrong = [0, 0, 0]\n",
    "    count_right = [0, 0, 0]\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] != true[i]:\n",
    "            if true[i] == 0:\n",
    "                count_wrong[0] += 1\n",
    "            if true[i] == 1:\n",
    "                count_wrong[1] +=1\n",
    "            if true[i] == 2:\n",
    "                count_wrong[2] += 1\n",
    "        else:\n",
    "            if true[i] == 0:\n",
    "                count_right[0] += 1\n",
    "            if true[i] == 1:\n",
    "                count_right[1] +=1\n",
    "            if true[i] == 2:\n",
    "                count_right[2] += 1\n",
    "    count_tot = list(np.add(count_wrong, count_right))\n",
    "    percent_wrong = list(np.divide(count_wrong,count_tot))\n",
    "    count_max = np.max(percent_wrong)\n",
    "    most_wrong = percent_wrong.index(count_max)\n",
    "    total = count_tot[most_wrong]\n",
    "    print(\"Class %d is mostly wrongly classified, with %.2f%% of the class wrongly put.\"%(most_wrong, count_max*100))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1ceb3d2f-64d0-4695-a7dd-8e994638a07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 32ms/step\n",
      "Regarding the model in the basic part, in the validation dataset:\n",
      "Class 2 is mostly wrongly classified, with 7.69% of the class wrongly put.\n"
     ]
    }
   ],
   "source": [
    "### Here I use the models trained in the basic part.\n",
    "preds_val_basic = transfer_model_unfreezed.predict(tf_validation_dataset)\n",
    "class_preds_val_basic = np.argmax(preds_val_basic.logits, axis=1)\n",
    "print(\"Regarding the model in the basic part, in the validation dataset:\")\n",
    "most_wrong_class(class_preds_val_basic, tokenized_dataset_val[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7b9f5224-8792-4c64-ad44-621eeeb8a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 31ms/step\n",
      "Regarding the model in the basic part, in the test dataset:\n",
      "Class 2 is mostly wrongly classified, with 11.30% of the class wrongly put.\n"
     ]
    }
   ],
   "source": [
    "preds_test_basic = transfer_model_unfreezed.predict(tf_test_dataset)\n",
    "class_preds_test_basic = np.argmax(preds_test_basic.logits, axis=1)\n",
    "print(\"Regarding the model in the basic part, in the test dataset:\")\n",
    "most_wrong_class(class_preds_test_basic, tokenized_dataset_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7e7b3c84-d3bc-43cb-a48b-d41f29cf7fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 30ms/step\n",
      "Regarding the model trained with class weights, in the validation dataset:\n",
      "Class 2 is mostly wrongly classified, with 17.09% of the class wrongly put.\n"
     ]
    }
   ],
   "source": [
    "### Here I use the models trained based on imbalance adjustment.\n",
    "preds_val_adjusted = transfer_model_imbalanced_unfreezed.predict(tf_validation_dataset)\n",
    "class_preds_val_adjusted = np.argmax(preds_val_adjusted.logits, axis=1)\n",
    "print(\"Regarding the model trained with class weights, in the validation dataset:\")\n",
    "most_wrong_class(class_preds_val_adjusted, tokenized_dataset_val[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9383725d-febe-46b7-8808-a87c31c28786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 30ms/step\n",
      "Regarding the model trained with class weights, in the test dataset:\n",
      "Class 2 is mostly wrongly classified, with 16.52% of the class wrongly put.\n"
     ]
    }
   ],
   "source": [
    "preds_test_adjusted = transfer_model_imbalanced_unfreezed.predict(tf_test_dataset)\n",
    "class_preds_test_adjusted = np.argmax(preds_test_adjusted.logits, axis=1)\n",
    "print(\"Regarding the model trained with class weights, in the test dataset:\")\n",
    "most_wrong_class(class_preds_test_adjusted, tokenized_dataset_test[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977be97-a6cb-4d77-9960-36f022295e71",
   "metadata": {},
   "source": [
    "### If there exist patterns in those sentences that are wrongly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "11abdae1-0bbb-4df5-a1d0-1cf346c3f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_index(pred, true):\n",
    "    result = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] != true[i]:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def find_right_index(pred, true):\n",
    "    result = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i]:\n",
    "            result.append(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "da8e4405-6f4b-4163-8642-5d580d85f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self, pred, true_dataset):\n",
    "        self.pred = pred\n",
    "        self.true_dataset = true_dataset\n",
    "        self.wrong_sentence = []\n",
    "        self.right_sentence = []\n",
    "\n",
    "    def length_comparison(self):\n",
    "        wrong_index = find_wrong_index(self.pred, self.true_dataset[\"label\"])\n",
    "        right_index = find_right_index(self.pred, self.true_dataset[\"label\"])\n",
    "        \n",
    "        # wrong_sentence = []\n",
    "        for i in range(len(wrong_index)):\n",
    "            self.wrong_sentence.append(\n",
    "                (self.true_dataset[\"sentence\"][wrong_index[i]],\n",
    "                 self.true_dataset[\"label\"][wrong_index[i]],\n",
    "                 self.pred[wrong_index[i]])\n",
    "            )\n",
    "            \n",
    "        # right_sentence = []\n",
    "        for i in range(len(right_index)):\n",
    "            self.right_sentence.append(self.true_dataset[\"sentence\"][right_index[i]])\n",
    "\n",
    "        avg_len1=0\n",
    "        for i in range(len(self.wrong_sentence)):\n",
    "            avg_len1 = (avg_len1*i + len(self.wrong_sentence[i][0]))/(i+1)\n",
    "        avg_len2=0\n",
    "        for i in range(len(self.right_sentence)):\n",
    "            avg_len2 = (avg_len2*i + len(self.right_sentence[i]))/(i+1)\n",
    "            \n",
    "        print(\"The average length of all the wrongly classified sentences is %.2f, the average lenghth of all the correctly classified sentences is %.2f.\"%(avg_len1, avg_len2))\n",
    "        return\n",
    "\n",
    "    def bias_analysis(self):\n",
    "        pos_bias = 0\n",
    "        neg_bias = 0\n",
    "        ### For each tuple, the first element is the wrongly put sentence, the second is the true class, third the prediction class\n",
    "        if not self.right_sentence or not self.wrong_sentence:\n",
    "            wrong_index = find_wrong_index(self.pred, self.true_dataset[\"label\"])\n",
    "            right_index = find_right_index(self.pred, self.true_dataset[\"label\"])\n",
    "            \n",
    "            for i in range(len(wrong_index)):\n",
    "                self.wrong_sentence.append(\n",
    "                    (self.true_dataset[\"sentence\"][wrong_index[i]],\n",
    "                     self.true_dataset[\"label\"][wrong_index[i]],\n",
    "                     self.pred[wrong_index[i]])\n",
    "                )\n",
    "                \n",
    "            for i in range(len(right_index)):\n",
    "                self.right_sentence.append(self.true_dataset[\"sentence\"][right_index[i]])\n",
    "        \n",
    "        for i in range(len(self.wrong_sentence)):\n",
    "            if self.wrong_sentence[i][1] < self.wrong_sentence[i][2]:\n",
    "                ### if the prediction class is more positive than the true class, the model outputs positive bias\n",
    "                pos_bias += 1\n",
    "            else:\n",
    "                neg_bias += 1\n",
    "        pos_bias = pos_bias/len(self.wrong_sentence)*100\n",
    "        neg_bias = neg_bias/len(self.wrong_sentence)*100\n",
    "        print(\"%.2f%% of the predictions suffer from positive bias, and %.2f%% of thepredictions suffers from negative bias.\"%(pos_bias,neg_bias))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "32bbb81c-c5a8-47ef-b101-b92a637beb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding the model in the basic part, in the validation dataset:\n",
      "The average length of all the wrongly classified sentences is 115.75, the average lenghth of all the correctly classified sentences is 122.83.\n",
      "37.50% of the predictions suffer from positive bias, and 62.50% of thepredictions suffers from negative bias.\n"
     ]
    }
   ],
   "source": [
    "print(\"Regarding the model in the basic part, in the validation dataset:\")\n",
    "val_basic = Analysis(class_preds_val_basic, tokenized_dataset_val)\n",
    "val_basic.length_comparison()\n",
    "val_basic.bias_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4d678a09-c35e-42bb-8e89-2e25277b0f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding the model in the basic part, in the test dataset:\n",
      "The average length of all the wrongly classified sentences is 151.72, the average lenghth of all the correctly classified sentences is 117.49.\n",
      "11.11% of the predictions suffer from positive bias, and 88.89% of thepredictions suffers from negative bias.\n"
     ]
    }
   ],
   "source": [
    "print(\"Regarding the model in the basic part, in the test dataset:\")\n",
    "test_basic = Analysis(class_preds_test_basic, tokenized_dataset_test)\n",
    "test_basic.length_comparison()\n",
    "test_basic.bias_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c05d9c31-32ce-42cc-90eb-ab132c2ca275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding the model trained with class weights, in the validation dataset:\n",
      "The average length of all the wrongly classified sentences is 129.43, the average lenghth of all the correctly classified sentences is 121.85.\n",
      "45.45% of the predictions suffer from positive bias, and 54.55% of thepredictions suffers from negative bias.\n"
     ]
    }
   ],
   "source": [
    "print(\"Regarding the model trained with class weights, in the validation dataset:\")\n",
    "val_adjusted = Analysis(class_preds_val_adjusted, tokenized_dataset_val)\n",
    "val_adjusted.length_comparison()\n",
    "val_adjusted.bias_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "189812da-c0c1-435f-aba3-1ce6a71f2401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding the model trained with class weights, in the test dataset:\n",
      "The average length of all the wrongly classified sentences is 137.40, the average lenghth of all the correctly classified sentences is 116.70.\n",
      "46.81% of the predictions suffer from positive bias, and 53.19% of thepredictions suffers from negative bias.\n"
     ]
    }
   ],
   "source": [
    "print(\"Regarding the model trained with class weights, in the test dataset:\")\n",
    "test_adjusted = Analysis(class_preds_test_adjusted, tokenized_dataset_test)\n",
    "test_adjusted.length_comparison()\n",
    "test_adjusted.bias_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184ad63-766f-4b84-b0b1-fd109ab6e2ca",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "In this section, I try to apply both the model before and after the class weights adjustment to superior error analysis. Specifically, I look into both the validation dataset and the test dataset, in order to compare the average length of sentences, and to see what kind of bias the models have when making classification. \n",
    "\n",
    "As we could see from the error analysis of the model both from the basic part and from the class-weights-adjusted model, generally, the wrongly classified sentences are longer than those correctly classified. It makes sense in that longer sentences contain more semantic meanings and are more complex to analyze for the model. The second pattern is that positive or neutral sentences are more likely to be wrongly classified to the more negative class. The most wrongly classified class for both models is class 2, which is the positive class, proving that the models suffer more from negative bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49084a2-0a9e-411a-80f3-8ba0981021b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment with different Pre-Trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582303b-a560-405d-b95d-d256308b99a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1cd7dac-815c-4aa2-b75b-736a87385083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9d821e7550479c8281849a19a04608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      " dropout_157 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset_train, tokenized_dataset_val, tokenized_dataset_test =train_val_test_split(tokenized_dataset, test_size=0.2, val_size=0.2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_dataset = tokenized_dataset_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_dataset_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_test_dataset = tokenized_dataset_test.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf4c23d-7c1e-4c4e-9504-ade01884797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 9s 29ms/step - loss: 0.5713 - accuracy: 0.7437 - val_loss: 0.3881 - val_accuracy: 0.8300\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.4484 - accuracy: 0.7946 - val_loss: 0.4505 - val_accuracy: 0.8102\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.3822 - accuracy: 0.8402 - val_loss: 0.3185 - val_accuracy: 0.8499\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.3614 - accuracy: 0.8365 - val_loss: 0.3894 - val_accuracy: 0.8190\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.3160 - accuracy: 0.8682 - val_loss: 0.3160 - val_accuracy: 0.8653\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.3131 - accuracy: 0.8682 - val_loss: 0.3587 - val_accuracy: 0.8653\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.2945 - accuracy: 0.8807 - val_loss: 0.3177 - val_accuracy: 0.8565\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.3074 - accuracy: 0.8741 - val_loss: 0.3397 - val_accuracy: 0.8477\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.2488 - accuracy: 0.8962 - val_loss: 0.3586 - val_accuracy: 0.8521\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.2532 - accuracy: 0.9021 - val_loss: 0.3217 - val_accuracy: 0.8565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23aeef20820>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model = createTransferModel(model,3)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fade5ca8-f781-48c7-bcd0-35f5ecf294b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8565121412803532"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val = tf.nn.softmax(transfer_model.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d17006ff-248e-4327-be19-8b65b8e7a2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8852097130242825"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = tf.nn.softmax(transfer_model.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf3fdcab-09d7-4b41-9abf-1be95fb9d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 17s 63ms/step - loss: 0.3329 - accuracy: 0.8962 - val_loss: 0.1448 - val_accuracy: 0.9536\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 10s 56ms/step - loss: 0.0895 - accuracy: 0.9705 - val_loss: 0.1292 - val_accuracy: 0.9492\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 10s 56ms/step - loss: 0.0409 - accuracy: 0.9860 - val_loss: 0.1590 - val_accuracy: 0.9382\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 9s 56ms/step - loss: 0.0255 - accuracy: 0.9904 - val_loss: 0.1330 - val_accuracy: 0.9625\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 10s 56ms/step - loss: 0.0090 - accuracy: 0.9956 - val_loss: 0.1928 - val_accuracy: 0.9603\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 9s 56ms/step - loss: 0.0983 - accuracy: 0.9750 - val_loss: 0.1875 - val_accuracy: 0.9536\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 10s 57ms/step - loss: 0.1270 - accuracy: 0.9654 - val_loss: 0.1556 - val_accuracy: 0.9470\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 9s 56ms/step - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.3302 - val_accuracy: 0.9448\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 10s 57ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 0.3378 - val_accuracy: 0.9272\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 10s 57ms/step - loss: 0.0119 - accuracy: 0.9948 - val_loss: 0.2493 - val_accuracy: 0.9514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23aeeee7eb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model_unfreezed = createTransferModel(transfer_model,freeze=False)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model_unfreezed.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model_unfreezed.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d14e163-d74b-4e6a-8f77-065d02afc472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9514348785871964"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val = tf.nn.softmax(transfer_model_unfreezed.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31962951-f033-43a0-b88b-eb6308e6aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9403973509933775"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = tf.nn.softmax(transfer_model_unfreezed.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4091362-24c8-4e56-b399-d2c24f285e88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Bertweet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd59208c-5294-4a15-8352-2f10e06c81df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 134309376 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592899    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,902,275\n",
      "Trainable params: 134,902,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"vinai/bertweet-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=False)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset_train, tokenized_dataset_val, tokenized_dataset_test =train_val_test_split(tokenized_dataset, test_size=0.2, val_size=0.2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_dataset = tokenized_dataset_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_dataset_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_test_dataset = tokenized_dataset_test.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "bertweet = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "bertweet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47f22797-2b9c-423b-b7d4-8117b86c864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 18s 52ms/step - loss: 0.6975 - accuracy: 0.7194 - val_loss: 0.5544 - val_accuracy: 0.7572\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 6s 36ms/step - loss: 0.5879 - accuracy: 0.7452 - val_loss: 0.4795 - val_accuracy: 0.7726\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 6s 37ms/step - loss: 0.5667 - accuracy: 0.7467 - val_loss: 0.4784 - val_accuracy: 0.7792\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 7s 40ms/step - loss: 0.5190 - accuracy: 0.7901 - val_loss: 0.4479 - val_accuracy: 0.8212\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 7s 39ms/step - loss: 0.5033 - accuracy: 0.8041 - val_loss: 0.4348 - val_accuracy: 0.8013\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 6s 37ms/step - loss: 0.5093 - accuracy: 0.7872 - val_loss: 0.4390 - val_accuracy: 0.8146\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 6s 37ms/step - loss: 0.4968 - accuracy: 0.7946 - val_loss: 0.4288 - val_accuracy: 0.7947\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 6s 37ms/step - loss: 0.4970 - accuracy: 0.7879 - val_loss: 0.5325 - val_accuracy: 0.7439\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 6s 37ms/step - loss: 0.4508 - accuracy: 0.8144 - val_loss: 0.3865 - val_accuracy: 0.8344\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 6s 38ms/step - loss: 0.4708 - accuracy: 0.8174 - val_loss: 0.5283 - val_accuracy: 0.7616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b0da372b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_bertweet = createTransferModel(bertweet,1)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_bertweet.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_bertweet.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b039bd7-58bb-4adf-b6c4-54d35dd7245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7615894039735099"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val = tf.nn.softmax(transfer_bertweet.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02becb43-23b7-4b09-aed7-f333459ee96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8123620309050773"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = tf.nn.softmax(transfer_bertweet.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cb0e23f-2e01-46bc-8752-f8172d694bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 53s 207ms/step - loss: 0.4943 - accuracy: 0.8675 - val_loss: 0.1872 - val_accuracy: 0.9448\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 32s 190ms/step - loss: 0.1620 - accuracy: 0.9529 - val_loss: 0.1641 - val_accuracy: 0.9492\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 34s 200ms/step - loss: 0.1705 - accuracy: 0.9529 - val_loss: 0.1232 - val_accuracy: 0.9647\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 35s 208ms/step - loss: 0.1697 - accuracy: 0.9492 - val_loss: 0.1859 - val_accuracy: 0.9382\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 35s 204ms/step - loss: 0.1362 - accuracy: 0.9595 - val_loss: 0.1871 - val_accuracy: 0.9581\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 34s 202ms/step - loss: 0.1172 - accuracy: 0.9683 - val_loss: 0.1419 - val_accuracy: 0.9625\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 34s 201ms/step - loss: 0.6449 - accuracy: 0.7555 - val_loss: 0.1942 - val_accuracy: 0.9404\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 35s 205ms/step - loss: 0.1521 - accuracy: 0.9551 - val_loss: 0.1672 - val_accuracy: 0.9603\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 37s 216ms/step - loss: 0.1134 - accuracy: 0.9691 - val_loss: 0.1071 - val_accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 36s 213ms/step - loss: 0.0541 - accuracy: 0.9860 - val_loss: 0.1939 - val_accuracy: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b460bef10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_bertweet_unfreezed = createTransferModel(transfer_bertweet,freeze=False)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_bertweet_unfreezed.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_bertweet_unfreezed.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "317c5000-d80e-4d51-9f32-74e70088fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 8s 82ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9448123620309051"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val = tf.nn.softmax(transfer_bertweet_unfreezed.predict(tf_validation_dataset)[\"logits\"])\n",
    "class_preds_val = np.argmax(preds_val, axis=1)\n",
    "accuracy_val = accuracy_score(tokenized_dataset_val[\"label\"],class_preds_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ca12aec-ac58-44e3-ba1f-8e5efc12a163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 5s 91ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9359823399558499"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = tf.nn.softmax(transfer_bertweet_unfreezed.predict(tf_test_dataset)[\"logits\"])\n",
    "class_preds_test = np.argmax(preds_test, axis=1)\n",
    "accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16070a8-d351-4e49-8adf-76a57b986fd9",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "To see if a larger/smaller model performs better, I first try distil-bert model, which has fewer parameters than the bert model in the basic part. Before fine-tuning, it actually achives better performance with a validation accuracy around 85% and a test accuracy around 88%. However, after unfreezing all the weights and fine-tuning, the bert model has better performance. This result makes sense because the bert model with all layers unfreezed has more parameters to be fine-tuned. A larger model again has greater learning capacity - its larger set of parameters can adapt better to new data. \n",
    "More parameters enable capturing more complex patterns when fine-tuning on downstream tasks\n",
    "\n",
    "To see if a model trained on different data will have a different performance, I try Bertweet model, which is trained on English tweets data. Before fine-tuning, it also has a better performance than Bert. However, after fine-tuning, the performance is not as good. The good performance before fine tuning cound be due to the fact that bertweet is a large model. The Bert model is trained on Wikipedia and book data, while the bertweet is trained on tweets data, which could be more casual and less compatible with financial news. Therefore, after fine-tuning, the performance of Bertweet is not as good as Bert.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28dba6-9917-48ec-85c0-5a44aeadebce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment with Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a21dd6a-05b4-438d-9bdf-362f9bd291af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 21s 58ms/step - loss: 0.8948 - accuracy: 0.6053 - val_loss: 0.7488 - val_accuracy: 0.6645\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 0.7676 - accuracy: 0.6811 - val_loss: 0.6853 - val_accuracy: 0.7373\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 0.7161 - accuracy: 0.7003 - val_loss: 0.6650 - val_accuracy: 0.7263\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.7010 - accuracy: 0.7047 - val_loss: 0.6052 - val_accuracy: 0.7373\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 8s 44ms/step - loss: 0.6596 - accuracy: 0.7231 - val_loss: 0.6031 - val_accuracy: 0.7859\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6502 - accuracy: 0.7216 - val_loss: 0.5775 - val_accuracy: 0.7616\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 8s 46ms/step - loss: 0.6448 - accuracy: 0.7371 - val_loss: 0.5886 - val_accuracy: 0.7594\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6242 - accuracy: 0.7342 - val_loss: 0.5557 - val_accuracy: 0.7660\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6144 - accuracy: 0.7474 - val_loss: 0.5493 - val_accuracy: 0.7726\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 8s 45ms/step - loss: 0.6065 - accuracy: 0.7327 - val_loss: 0.5132 - val_accuracy: 0.8013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17994bae3d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Data \n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "dataset_1 = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", split = \"train\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True, padding=True)\n",
    "\n",
    "def train_val_test_split(tokenized_dataset, test_size, val_size=0):\n",
    "        temp1 = tokenized_dataset.train_test_split(test_size = test_size)\n",
    "        tokenized_test = temp1[\"test\"]\n",
    "        temp2 = temp1[\"train\"]\n",
    "        temp3 = temp2.train_test_split(test_size = val_size/(1-test_size))\n",
    "        tokenized_val = temp3[\"test\"]\n",
    "        tokenized_train = temp3[\"train\"]\n",
    "        return tokenized_train, tokenized_val, tokenized_test\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_dataset = dataset_1.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset_train, tokenized_dataset_val, tokenized_dataset_test =train_val_test_split(tokenized_dataset, test_size=0.2, val_size=0.2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_dataset = tokenized_dataset_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_dataset_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_test_dataset = tokenized_dataset_test.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model_sampling = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "transfer_model_sampling = createTransferModel(model_sampling,1)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "# optimizer = AdamWeightDecay(transfer_model.config)\n",
    "transfer_model_sampling.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "transfer_model_sampling.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs = 10,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06924f23-66f2-41b0-9b6d-90ed7a1a0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def fine_tuning_with_sampling(model, sample_size, class_weights=None):\n",
    "    pd_tokenized_dataset_train = tokenized_dataset_train.to_pandas()\n",
    "    if class_weights == None:\n",
    "        pd_sampled = pd_tokenized_dataset_train.sample(sample_size)\n",
    "    else:\n",
    "        pd_sampled = pd_tokenized_dataset_train.sample(sample_size,weights=class_weights)\n",
    "    dset = Dataset.from_pandas(pd_sampled)\n",
    "    tf_dset = dset.to_tf_dataset(\n",
    "        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "        label_cols=[\"label\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    model_unfreezed = createTransferModel(model, freeze=False)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model_unfreezed.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "        loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model_unfreezed.fit(\n",
    "        tf_dset,\n",
    "        validation_data=tf_validation_dataset,\n",
    "        epochs = 15,\n",
    "        callbacks = [callback]\n",
    "    )\n",
    "    preds_test = tf.nn.softmax(model_unfreezed.predict(tf_test_dataset)[\"logits\"])\n",
    "    class_preds_test = np.argmax(preds_test, axis=1)\n",
    "    accuracy_test = accuracy_score(tokenized_dataset_test[\"label\"],class_preds_test)\n",
    "    return accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c87cb3-2fda-4f89-9ea7-92f23c253721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def calculate_prob(data):\n",
    "    weights = []\n",
    "    pd_data = data.to_pandas()\n",
    "    tot_length = len(pd_data)\n",
    "    neg_prob = len(pd_data[pd_data[\"label\"]==0][\"label\"])/tot_length\n",
    "    neu_prob = len(pd_data[pd_data[\"label\"]==1][\"label\"])/tot_length\n",
    "    pos_prob = len(pd_data[pd_data[\"label\"]==2][\"label\"])/tot_length\n",
    "    label_list = list(pd_data[\"label\"])\n",
    "    for i in range(len(label_list)):\n",
    "        if label_list[i] == 0:\n",
    "            weights.append(neg_prob)\n",
    "        elif label_list[i] == 1:\n",
    "            weights.append(neu_prob)\n",
    "        else:\n",
    "            weights.append(pos_prob)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e909226a-4a38-4637-b31b-d2572875111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = calculate_prob(tokenized_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99077090-5391-463b-a99e-454373a8bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 21s 173ms/step - loss: 0.4621 - accuracy: 0.7950 - val_loss: 0.2990 - val_accuracy: 0.8962\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 0.1935 - accuracy: 0.9250 - val_loss: 0.1833 - val_accuracy: 0.9272\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 6s 125ms/step - loss: 0.0674 - accuracy: 0.9800 - val_loss: 0.1790 - val_accuracy: 0.9404\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 0.0316 - accuracy: 0.9975 - val_loss: 0.1661 - val_accuracy: 0.9360\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 0.0147 - accuracy: 0.9975 - val_loss: 0.1746 - val_accuracy: 0.9404\n",
      "Epoch 6/15\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9470\n",
      "Epoch 7/15\n",
      "50/50 [==============================] - 6s 125ms/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 0.1925 - val_accuracy: 0.9360\n",
      "Epoch 8/15\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9316\n",
      "Epoch 9/15\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1781 - val_accuracy: 0.9382\n",
      "57/57 [==============================] - 4s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9470198675496688"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_with_sampling(model=transfer_model_sampling, sample_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf74fce-e2a3-4134-a318-e2b81e0f8113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 24s 131ms/step - loss: 0.0275 - accuracy: 0.9912 - val_loss: 0.1189 - val_accuracy: 0.9691\n",
      "Epoch 2/15\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 0.1817 - val_accuracy: 0.9514\n",
      "Epoch 3/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.1176 - val_accuracy: 0.9713\n",
      "Epoch 4/15\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 7.9688e-04 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9713\n",
      "Epoch 5/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 4.4516e-04 - accuracy: 1.0000 - val_loss: 0.1142 - val_accuracy: 0.9757\n",
      "Epoch 6/15\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 3.1499e-04 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9735\n",
      "Epoch 7/15\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 2.2923e-04 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9757\n",
      "Epoch 8/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 2.0306e-04 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 0.9757\n",
      "57/57 [==============================] - 4s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9558498896247241"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_with_sampling(model=transfer_model_sampling, sample_size=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836ba9e9-f64f-45a4-9dee-d27c6f13dbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 29s 119ms/step - loss: 0.1607 - accuracy: 0.9517 - val_loss: 0.0970 - val_accuracy: 0.9669\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 16s 104ms/step - loss: 0.0337 - accuracy: 0.9892 - val_loss: 0.1111 - val_accuracy: 0.9603\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 16s 104ms/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.1253 - val_accuracy: 0.9492\n",
      "Epoch 4/15\n",
      "150/150 [==============================] - 15s 103ms/step - loss: 0.0054 - accuracy: 0.9992 - val_loss: 0.1234 - val_accuracy: 0.9558\n",
      "57/57 [==============================] - 5s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9646799116997793"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_with_sampling(model=transfer_model_sampling, sample_size=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1acc83b0-4665-4200-a619-0a7044a37955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanji\\miniconda3\\envs\\py39\\lib\\site-packages\\datasets\\arrow_dataset.py:399: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 25s 131ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.1279 - val_accuracy: 0.9735\n",
      "Epoch 2/15\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.1291 - val_accuracy: 0.9669\n",
      "Epoch 3/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0146 - accuracy: 0.9962 - val_loss: 0.0984 - val_accuracy: 0.9713\n",
      "Epoch 4/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0051 - accuracy: 0.9975 - val_loss: 0.1594 - val_accuracy: 0.9603\n",
      "Epoch 5/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 3.5893e-04 - accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9669\n",
      "Epoch 6/15\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.9739e-04 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9669\n",
      "57/57 [==============================] - 4s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9646799116997793"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_with_sampling(model=transfer_model_sampling, sample_size=800,class_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c6e89-bcf6-4343-94ea-81c69e012335",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "To see how the out-of-sample performance changes with the length of the training dataset in the process of fine-tuning, I write a fine_tuning_with_sampling function to help. The first sampling method is randomly taking a fixed sample size of data from the whole training dataset. Then, we fine tune the model based on the sampled data. As we could see from above, **the out-of-sample performance increases with the sample size**. The second sampling method is to make sure that each class has the sample probability of being sampled. Namely, we calculate the prior probability of each class in the training dataset, and assign this probability to sampling. **By sampling with class weights considered, we can see that the fine-tuned model has a better performance with the same sample size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485d1c7-5999-4745-86c2-8ca8f82d118a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## In-context learning\n",
    "\n",
    "*Relative difficulty: low but fun !*\n",
    "\n",
    "Can you use few-shot learning successfully (i.e., no further training) ?\n",
    "\n",
    "It would be great to do this for Financial PhraseBank but the sentences may be too long\n",
    "- pre-trained models have maximum sequence lengths that may be too small\n",
    "\n",
    "Propose some interesting task related to Finance and try to achieve Few Shot Learning on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a242cb58-6a07-488f-b41b-301789e9de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing . neutral\n",
    "   For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m . positive\n",
    "   The OMX Helsinki index was down 0.34 pct at 8,256.02 on turnover of 813.191 mln eur . negative\n",
    "\n",
    "   The growth of net sales has continued favourably in the Middle East and Africaand in Asia Pacific .\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d30e279-e8d8-41a0-a0bd-a934a6bed916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_input = tokenizer(input_text, return_tensors = \"tf\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78be15ce-18e9-45d0-afbb-c45cb0fa5661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint,num_labels=3)\n",
    "preds = model.predict(tokenized_input)[\"logits\"]\n",
    "class_preds = np.argmax(preds, axis=1)\n",
    "class_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586e377-8b86-4c4d-8594-f48697a6aa7f",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Using few-shot learning, we do not train the model and try to feed some contexts to the model and hopefully we could get the prediction. Here, I feed three classified sentences as context and one sentence (should be positive) to the model. Without any further training, the model successfully predicts that the sentence is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ace91-b7fc-4541-83c1-99a6af764f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789edd8-981e-415d-a50c-3083b4e67807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
